{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval evaluation, graded relevance\n",
    "\n",
    "  - Compute retrieval evaluation metrics using graded relevance: NDCG@5 and NDCG@10\n",
    "  - Compute the metrics for each query individually, as well as the averages over the entire query set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rankings produced for each query\n",
    "\n",
    "The key is the queryID, the value is a list of docIDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rankings = {\n",
    "    \"q1\": [2, 1, 3, 4, 5, 6, 10, 7, 9, 8],\n",
    "    \"q2\": [1, 2, 9, 4, 5, 6, 7, 8, 3, 10],\n",
    "    \"q3\": [1, 7, 4, 5, 3, 6, 9, 8, 10, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground truth\n",
    "\n",
    "The key is the queryID, the value is a dictionary with (docID, level) pairs. Relevance level is on a 3-point scale: non-relevant (0), poor (1), good (2), excellent (3). Documents not listed here are non-relevant (level=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gtruth = {\n",
    "    \"q1\": {4: 3, 1: 2, 2: 1},\n",
    "    \"q2\": {3: 3, 4: 3, 1: 2, 2: 1, 8: 1},\n",
    "    \"q3\": {1: 3, 4: 3, 7: 2, 5: 2, 6: 1, 8: 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing evaluation metrics\n",
    "\n",
    "Discounted cumulative gain at rank p:\n",
    "$DCG_p = rel_1 + \\sum_{i=2}^p\\frac{rel_i}{\\log_2 i}$\n",
    "\n",
    "Normalized discounted cumulative gain at rank p:\n",
    "$NDCG_p = \\frac{DCG_p}{IDCG}$\n",
    "\n",
    "where IDCG is the DCG_p score of an idealized (perfect) ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function that computes DCG_p\n",
    "\n",
    "It takes a list of relevance levels (corresponding to the documents) and the rank cutoff p as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dcg(rel, p):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating q1\n",
      "2(1)\n",
      "1(2)\n",
      "3(0)\n",
      "4(3)\n",
      "5(0)\n",
      "6(0)\n",
      "10(0)\n",
      "7(0)\n",
      "9(0)\n",
      "8(0)\n",
      "Evaluating q2\n",
      "1(2)\n",
      "2(1)\n",
      "9(0)\n",
      "4(3)\n",
      "5(0)\n",
      "6(0)\n",
      "7(0)\n",
      "8(1)\n",
      "3(3)\n",
      "10(0)\n",
      "Evaluating q3\n",
      "1(3)\n",
      "7(2)\n",
      "4(3)\n",
      "5(2)\n",
      "3(0)\n",
      "6(1)\n",
      "9(0)\n",
      "8(1)\n",
      "10(0)\n",
      "2(0)\n"
     ]
    }
   ],
   "source": [
    "for qid, ranking in sorted(rankings.items()):\n",
    "    gt = gtruth[qid]    \n",
    "    print(\"Evaluating\", qid)\n",
    "    # printing gain levels for each document\n",
    "    for doc_id in ranking: \n",
    "        gain = gt.get(doc_id, 0)\n",
    "        print(str(doc_id) + \"(\" + str(gain) + \")\")\n",
    "    \n",
    "    # TODO\n",
    "    # - compute NDCG@5\n",
    "    # - compute NDCG@10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
